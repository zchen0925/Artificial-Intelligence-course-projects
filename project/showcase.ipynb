{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "######  fMRI Decoding Project Showcase\n\n \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "First subject functional nifti images (4D) are at: C:\\Users\\Ziqi/nilearn_data\\haxby2001\\subj1\\bold.nii.gz\nNumber of trials:  324\nC:\\Users\\Ziqi/nilearn_data\\haxby2001\\mask.nii.gz\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "from nilearn import datasets, image, plotting\nfrom nilearn.input_data import NiftiMasker\nfrom nilearn.image.image import mean_img\nfrom nilearn.image import index_img\nimport pandas as pd\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Preprocessing the Haxby Dataset\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "First subject functional nifti images (4D) are at: C:\\Users\\Ziqi/nilearn_data\\haxby2001\\subj1\\bold.nii.gz\nNumber of trials:  324\nC:\\Users\\Ziqi/nilearn_data\\haxby2001\\mask.nii.gz\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "#import Haxby et al.(2001): Faces and Objects in Ventral Temporal Cortex (fMRI)\n# Subjects 5 and 6 don\u0027t have complete label or anatomical information, only included subjects 1-4\nhaxby_dataset \u003d datasets.fetch_haxby(subjects\u003d4)\n\n#load nifti images for the given subjects. Range 0-3\n#defaults to subject 2\ndef loadSubject(subjectNum \u003d 1):\n    # \u0027func\u0027 is a list of filenames: one for each subject\n    fmri_filename \u003d haxby_dataset.func[subjectNum]\n    # print basic information on the dataset\n    print(\u0027First subject functional nifti images (4D) are at: %s\u0027 %\n          fmri_filename)  # 4D data\n    return fmri_filename\n\n#plotting subject\u0027s anatomical brain\ndef plotAnat(subjectNum \u003d 1):\n    path \u003d haxby_dataset.anat[subjectNum]\n    plotting.plot_stat_map(path, threshold\u003d3)\n    plotting.show()\n\n#plotting mean functionam MRI\ndef plotMeanFunc(subjectNum \u003d 1):\n    mean_haxby \u003d mean_img(haxby_dataset.func[subjectNum])\n    plotting.plot_stat_map(mean_haxby, threshold\u003d3)\n    plotting.show()\n\n#plotting one random scan of fMRI\ndef plotRandomFunc(subjectNum \u003d 1):\n    rand_func \u003d index_img(haxby_dataset.func[subjectNum], 30)\n    plotting.plot_stat_map(rand_func, threshold\u003d3)\n    plotting.show()\n\nfmri_filename \u003d loadSubject(0)\n# plotAnat(subjectNum \u003d 2)\n# plotMeanFunc(2)\n# plotRandomFunc(2)\n\nbehavioral \u003d pd.read_csv(haxby_dataset.session_target[0], sep\u003d\" \")\nconditions \u003d behavioral[\u0027labels\u0027]\n\nfacecat_mask \u003d conditions.isin([\u0027face\u0027, \u0027cat\u0027])\nconditions_facecat \u003d conditions[facecat_mask]\nsession_facecat \u003d behavioral[facecat_mask].to_records(index \u003d False)\n\nfacehouse_mask \u003d conditions.isin([\u0027face\u0027, \u0027house\u0027])\nconditions_facehouse \u003d conditions[facehouse_mask]\nsession_facehouse \u003d behavioral[facehouse_mask].to_records(index \u003d False)\n\nthreeway_mask \u003d conditions.isin([\u0027face\u0027, \u0027house\u0027, \u0027cat\u0027])\nconditions_threeway \u003d conditions[threeway_mask]\nsession_threeway \u003d behavioral[threeway_mask].to_records(index \u003d False)\nprint(\"Number of trials: \", len(conditions_threeway))\nmask_filename \u003d haxby_dataset.mask\n#masking the data from 4D image to 2D array: voxel x time\n#with smothing and standardization\nmasker \u003d NiftiMasker(mask_img\u003dmask_filename, smoothing_fwhm\u003d4, standardize\u003dTrue, memory\u003d\"nilearn_cache\", memory_level\u003d1)\nprint(haxby_dataset.mask)\nX \u003d masker.fit_transform(fmri_filename)\n\n# Apply our condition_mask\nFC \u003d X[facecat_mask]\n\nFH \u003d X[facehouse_mask]\n\nFHC \u003d X[threeway_mask]\n#three-way classification with NN\nFHC_train \u003d FHC[:250]\nconditions_train \u003d conditions_threeway[1:250]\nFHC_val \u003d FHC[250:]\nY_val \u003d conditions_threeway[250:]\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-3-fc9950760b9f\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mloadingData\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectPercentile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_classif\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named \u0027loadingData\u0027"
          ],
          "ename": "ModuleNotFoundError",
          "evalue": "No module named \u0027loadingData\u0027",
          "output_type": "error"
        }
      ],
      "source": "from loadingData import *\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import SelectPercentile, f_classif, SelectKBest\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom nilearn import image\nfrom nilearn.plotting import plot_stat_map, show\nfrom sklearn.model_selection import LeaveOneGroupOut, cross_val_score\nfrom sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n\n# Define the dimension reduction to be used.\n# Here we use a classical univariate feature selection based on F-test,\n# namely Anova. When doing full-brain analysis, it is better to use\n# SelectPercentile, keeping 5% of voxels\n# (because it is independent of the resolution of the data).\nfeature_selection \u003d SelectPercentile(f_classif, percentile\u003d5)\n\n#one-vs-the-rest\n#cited from https://scikit-learn.org/stable/modules/svm.html#multi-class-classification\nlin_svc \u003d LinearSVC()\nfacecathouse_svc \u003d Pipeline([(\u0027anova\u0027, feature_selection), (\u0027svc\u0027, lin_svc)])\nfacecathouse_svc.fit(FHC, conditions_threeway)\n\nanother_svc \u003d OneVsRestClassifier(Pipeline([(\u0027anova\u0027, SelectKBest(f_classif, k\u003d500)), (\u0027svc\u0027, SVC(kernel \u003d \u0027linear\u0027))]))\nanother_svc.fit(FHC, conditions_threeway)\n\n# Output accuracy\n# Define the cross-validation scheme used for validation.\n# Here we use a LeaveOneGroupOut cross-validation on the session group\n# which corresponds to a leave-one-session-out\ndef modelAccuracy(model, X, conditions, groups):\n    cv \u003d LeaveOneGroupOut()\n\n    # Compute the prediction accuracy for the different folds (i.e. session)\n    cv_scores \u003d cross_val_score(model, X, conditions, cv\u003dcv, groups\u003dgroups)\n\n    # Return the corresponding mean prediction accuracy\n    classification_accuracy \u003d cv_scores.mean()\n\n    # Print the results\n    print(\"Classification accuracy: %.4f / Chance level: %f\" %\n          (classification_accuracy, 1. / len(conditions.unique())))\n\nprint(\"Linear model on face vs cat vs house: \")\nmodelAccuracy(facecathouse_svc, FHC, conditions_threeway, session_threeway)\n\nprint(\"The second model on face vs cat vs house: \")\nmodelAccuracy(another_svc, FHC, conditions_threeway, session_threeway)\n\ncross_validation \u003d cross_val_score(facecathouse_svc, FHC, conditions_threeway, cv \u003d 5, verbose \u003d 1)\nprint(\"Linear kernel model cross validation score: \", cross_validation.mean())\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Using an NN model",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-4-c4bbeeb1021a\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mloadingData\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named \u0027loadingData\u0027"
          ],
          "ename": "ModuleNotFoundError",
          "evalue": "No module named \u0027loadingData\u0027",
          "output_type": "error"
        }
      ],
      "source": "import numpy as np\nfrom loadingData import *\nfrom keras import models\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n#need to one-hot encode the Y labels\nenc \u003d OneHotEncoder()\n#cited from https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\nY \u003d enc.fit_transform(conditions_threeway[:, np.newaxis]).toarray()\nlabels_train \u003d Y[:250]\nlabels_val \u003d Y[250:]\n\n\n# experimental attempt to transform the 2D voxel*time array\nreshaped_X \u003d np.empty(shape \u003d (324, 798))\nreshaped_dimension \u003d np.empty(shape \u003d (1, 798))\nsubject \u003d []\ndim \u003d []\n# for dimension in range(0, 324):\n#     for x in range(0, 797):\n#         sum_series \u003d 0\n#         for i in range(0, 49):\n#             index \u003d x * 50 + i\n#             sum_series +\u003d FHC[dimension][index]\n#         dim.append(sum_series/49)\n#     subject.append(dim)\n#\n# reshaped_X \u003d np.array(subject)\n\nfor i in range(0, 324):\n    old_dim \u003d FHC[i]\n    new_dim \u003d np.mean(old_dim[:(len(old_dim)// 50) * 50].reshape(-1, 50), axis\u003d1)\n    subject.append(new_dim)\n\nreshaped_X \u003d np.array(subject)\nprint(reshaped_X.shape)\nprint(reshaped_X)\n\nreshaped_Xtrain \u003d reshaped_X[:250]\nreshaped_Xval \u003d reshaped_X[250:]\n\n# model \u003d models.Sequential()\n# # model.add(Dense(64, input_dim \u003d 39912, activation\u003d\u0027relu\u0027))\n# # model.add(Dense(32, input_dim \u003d 64, activation\u003d\u0027relu\u0027))\n# model.add(Dense(16, input_dim \u003d 39912, activation\u003d\u0027relu\u0027))\n# # model.add(Dense(8, input_dim\u003d16, activation\u003d\u0027relu\u0027))\n# model.add(Dense(3, activation\u003d\u0027softmax\u0027))\n# model.summary()\n#\n# model.compile(optimizer\u003d\u0027adam\u0027, loss\u003d\u0027categorical_crossentropy\u0027, metrics\u003d[\u0027accuracy\u0027])\n# model.fit(FHC_train, labels_train, batch_size\u003d5, epochs\u003d50, verbose\u003d1)\n# score \u003d model.evaluate(FHC_val, labels_val)\n#\n# print(\u0027Test loss:\u0027, score[0])\n# print(\u0027Test accuracy:\u0027, score[1])\n\nsmaller_model \u003d models.Sequential()\n# smaller_model.add(Dense(64, input_dim \u003d 798, activation\u003d\u0027relu\u0027))\n# smaller_model.add(Dense(32, input_dim \u003d 64, activation\u003d\u0027relu\u0027))\nsmaller_model.add(Dense(16, input_dim \u003d 798, activation\u003d\u0027relu\u0027))\nsmaller_model.add(Dense(8, input_dim\u003d16, activation\u003d\u0027relu\u0027))\nsmaller_model.add(Dense(3, activation\u003d\u0027softmax\u0027))\nsmaller_model.summary()\n\nsmaller_model.compile(optimizer\u003d\u0027adam\u0027, loss\u003d\u0027categorical_crossentropy\u0027, metrics\u003d[\u0027accuracy\u0027])\nsmaller_model.fit(reshaped_Xtrain, labels_train, batch_size\u003d20, epochs\u003d50, verbose\u003d1)\nscore \u003d smaller_model.evaluate(reshaped_Xval, labels_val)\n\nprint(\u0027Model with reduced input: \u0027)\nprint(\u0027Test loss:\u0027, score[0])\nprint(\u0027Test accuracy:\u0027, score[1])\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}