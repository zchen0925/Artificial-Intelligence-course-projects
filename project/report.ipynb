{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "\u003ch4\u003e\u003ccenter\u003e AI Project Proposal Draft 2 \u003c/h4\u003e\n\u003ch5\u003eZiqi Chen\u003cbr\u003e\nCS 344\u003cbr\u003e\nMay 3rd, 2019\u003c/h5\u003e\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\u003ch5\u003eVision — Give an overview of the project and its purpose.\u003c/h5\u003e\nThis project was inspired from the neuroscience principle that our visual cortex is a complex system that designates specialized area or group of areas for the processing of different types of visual stimuli. Based on this idea, the activation patterns in our brains should be unique when we are seeing different classes of visual stimuli, such as a human face versus a cat versus an inanimate object.  \nWith the recent advance of neuroimaging technologies, we have more tools to shed light upon how our brain works. The brain scan data is piling up, but it isn\u0027t always easy for humans to pick apart the patterns and connections hidden in the fMRI images right away.  \nIn this project, I am interested to learn how to apply the strengths of Machine-Learning models we\u0027ve learned about in class: pattern-extraction, to simple analysis of neuroimagine data. This project trains ML models on a classification task, one that uses a form of represented fMRI data to find hidden distinct brain activation patterns that result from seeing a class of visual stimuli, and predicts the type of stimuli the person was seeing when this brain activity was recorded. \n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\u003ch5\u003eBackground\u003c/h5\u003e\nThis project aims to do a multiclass (3) classification of brain imaging samples. I\u0027m using the [Haxby et al (2001) dataset](https://zenodo.org/record/1203329#.XNpTO0MpBqs) to perform a 3-way classification, trying to train the models to distinguish the fMRI brain scans that result from seeing a face, a cat, or a house. The dataset contains block-design fMRI data for 6 subjects who viewed 12 runs of repeated visual presentations of various stimuli. For the purpose of this project, only subjects 1 - 4 were included as they have complete fMRI data and the corresponding text labels, describing the stimuli type used in each trial. \nIn terms of fMRI data manipulation, This project relies on NiLearn, a Python Scikit-Learn based library with high-level functions for manipulation and analysis of neuroimaging data. In particular, I reference the [documentation on the NiftiMasker class](http://nilearn.github.io/modules/generated/nilearn.input_data.NiftiMasker.html) which does the heavy-lifting of readying the fMRI data to be used by models. It applies a mask to 4D fMRI images to extract 2D arrays, with each datapoint representing voxel of brain region activation * time. In loadingData.py, I cited [this NiLearn example](http://nilearn.github.io/auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py) to create a data matrix from the fMRIs.\nIn terms of technologies used, I experimented with a simple [DNN](https://github.com/kvlinden-courses/cs344-code/blob/master/u08features/keras-mnist.ipynb) and [CNN](https://github.com/kvlinden-courses/cs344-code/blob/master/u09classification/keras-cnn.ipynb) models. Neural networks, especially Convolutional Neural Networks, are known for image-processing and classification, hence the inclusion in this project.  \nLast but not least, I used Linear Support Vector Machines as one of the models. Support Vector Machines are a class of supervised machine learning methods for classification and regression tasks. SVMs represent the training data as points in space (cited from [https://en.wikipedia.org/wiki/Support-vector_machine], and use a subset of training points in the decision function, also called support vectors. Its basic philosophy is to find the hyper-plane that most efficiently separates each class of samples from each other (cited from [https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/].  \nFrom talking with Professor Vander Linden during the walkthrough, I learned that Support Vector Machines have been studied for a couple decades, and they have strong support for smaller datasets. Considering the size of this dataset, SVMs seem a good choice. Additionally, Support Vector Classification is known for its strength in classification problems that have high dimensional X inputs (cited from [https://scikit-learn.org/stable/modules/svm.html]), and that is the case in this project. It is effective not only in binary classification, but also multiclass classification problems. For the latter, SVMs supports two types of approaches: one-vs-rest or one-vs-one. One-vs-rest establishes one class and compares it against the other, whereas one-vs-one compares every class with every other, hence more computationally demanding (cited from [https://stats.stackexchange.com/questions/142325/svm-three-way-classification]. This project uses the former approach, one-vs-rest. The implementation of this model is based on [https://scikit-learn.org/stable/modules/svm.html#multi-class-classification].  \nThe fine-tuning of SVM hyperparameters came from documentation on the set__params method from [this page](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\u003ch5\u003eImplementation — Summarize your implementation and, if appropriate, how it extends on the work you’ve referenced\u003c/h5\u003e\nGiven NiLearn\u0027s high-level pre-built functions, some of the implementation details will be carried out automatically in \nthe library.\nAfter reading tutorials, I think that the first step in implementation is downloading the dataset which will then be loaded\ninto Python. Next, I\u0027ll use NiLearn functions transforming 3D images with a time series into 2D arrays that Scikit accepts.\nThen a Support Vector Machine estimator will be trained on the data for the first two tasks. The parameters will be tweaked\nto achieve a satisfiable accuracy. Then, I\u0027ll need to research NiLearn functions that will plot the resulting brain activities\nfrom the prediction result arrays. Finally, for the third task, a correlation matrix need to be created based on time series data.\nA GraphicLasso estimator will be fit on the correlation matrix. I will look for existing projects on functional connectivity maps\nand compare the model\u0027s prediction with their findings. ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\u003ch5\u003eResults — Give the results of your system and comparing them with other similar work\u003c/h5\u003e\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Loading and Preprocessing of data**  \nHere I first download the Haxby dataset and then use the NiftiMasker class from NiLearn to preprocess the 4D fMRI images into numpy matrices. ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# loadingData.py employs the NiLearn Python library to download the Haxby dataset,\n# which contains the brain scan images of four subjects and the accompanying labels\n# of the images they looked at while their brain activity was captured.\n# NiftiMasker function in the NiLearn library was used to transform the 4D brain scan images into 2D Numpy arrays,\n# which are vectors in which each datapoint represents the extrapolated brain tissue voxel * time\n\nfrom nilearn import datasets, image, plotting\nfrom nilearn.input_data import NiftiMasker\nfrom nilearn.image.image import mean_img\nfrom nilearn.image import index_img\nimport numpy as np\nimport pandas as pd\n\n#import Haxby et al.(2001): Faces and Objects in Ventral Temporal Cortex (fMRI)\n# Subjects 5 and 6 don\u0027t have complete label or anatomical information, only included subjects 1-4\nhaxby_dataset \u003d datasets.fetch_haxby(subjects\u003d4)\n\n#load nifti images for the given subjects. Range 0-3\n#defaults to subject 2\ndef loadSubject(subjectNum \u003d 1):\n    # \u0027func\u0027 is a list of filenames: one for each subject\n    fmri_filename \u003d haxby_dataset.func[subjectNum]\n    # print basic information on the dataset\n    print(\u0027First subject functional nifti images (4D) are at: %s\u0027 %\n          fmri_filename)  # 4D data\n    return fmri_filename\n\n#plotting subject\u0027s anatomical brain\ndef plotAnat(subjectNum \u003d 1):\n    path \u003d haxby_dataset.anat[subjectNum]\n    plotting.plot_stat_map(path, threshold\u003d3)\n    plotting.show()\n\n#plotting mean functionam MRI\ndef plotMeanFunc(subjectNum \u003d 1):\n    mean_haxby \u003d mean_img(haxby_dataset.func[subjectNum])\n    plotting.plot_stat_map(mean_haxby, threshold\u003d3)\n    plotting.show()\n\n#plotting one random scan of fMRI\ndef plotRandomFunc(subjectNum \u003d 1):\n    rand_func \u003d index_img(haxby_dataset.func[subjectNum], 30)\n    plotting.plot_stat_map(rand_func, threshold\u003d3)\n    plotting.show()\n\nfmri_filename \u003d loadSubject(0)\n# plotAnat(subjectNum \u003d 2)\n# plotMeanFunc(2)\n# plotRandomFunc(2)\n\nbehavioral \u003d pd.read_csv(haxby_dataset.session_target[0], sep\u003d\" \")\nconditions \u003d behavioral[\u0027labels\u0027]\n\nfacecat_mask \u003d conditions.isin([\u0027face\u0027, \u0027cat\u0027])\nconditions_facecat \u003d conditions[facecat_mask]\nsession_facecat \u003d behavioral[facecat_mask].to_records(index \u003d False)\n\nfacehouse_mask \u003d conditions.isin([\u0027face\u0027, \u0027house\u0027])\nconditions_facehouse \u003d conditions[facehouse_mask]\nsession_facehouse \u003d behavioral[facehouse_mask].to_records(index \u003d False)\n\nthreeway_mask \u003d conditions.isin([\u0027face\u0027, \u0027house\u0027, \u0027cat\u0027])\nconditions_threeway \u003d conditions[threeway_mask]\nsession_threeway \u003d behavioral[threeway_mask].to_records(index \u003d False)\n# print(\"Number of trials: \", len(conditions_threeway))\nmask_filename \u003d haxby_dataset.mask\n\n#masking the data from 4D image to 2D array: voxel x time\n#with smothing and standardization\nmasker \u003d NiftiMasker(mask_img\u003dmask_filename, smoothing_fwhm\u003d4, standardize\u003dTrue, memory\u003d\"nilearn_cache\", memory_level\u003d1)\nX \u003d masker.fit_transform(fmri_filename)\n\n# Apply our condition_mask to subject 1\u0027s brain scans:\nFC \u003d X[facecat_mask]\n\nFH \u003d X[facehouse_mask]\n\nFHC \u003d X[threeway_mask]\n\n# References\n# Haxby, J., Gobbini, M., Furey, M., Ishai, A., Schouten, J., and Pietrini, P. (2001). Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science 293, 2425-2430.\n\n\ndef processSubject(sub):\n    mask_filename \u003d haxby_dataset.mask\n    # masking the data from 4D image to 2D array: voxel x time\n    # with smothing and standardization\n    masker \u003d NiftiMasker(mask_img\u003dmask_filename, smoothing_fwhm\u003d4, standardize\u003dTrue, memory\u003d\"nilearn_cache\",\n                         memory_level\u003d1)\n    X \u003d masker.fit_transform(loadSubject(sub))\n    behavioral \u003d pd.read_csv(haxby_dataset.session_target[sub], sep\u003d\" \")\n    conditions \u003d behavioral[\u0027labels\u0027]\n    threeway_mask \u003d conditions.isin([\u0027face\u0027, \u0027house\u0027, \u0027cat\u0027])\n    conditions_threeway \u003d conditions[threeway_mask]\n    FHC \u003d X[threeway_mask]\n    return FHC, conditions_threeway\n\ndef processSessions(sub):\n    behavioral \u003d pd.read_csv(haxby_dataset.session_target[sub], sep\u003d\" \")\n    conditions \u003d behavioral[\u0027labels\u0027]\n    threeway_mask \u003d conditions.isin([\u0027face\u0027, \u0027house\u0027, \u0027cat\u0027])\n    session_threeway \u003d behavioral[threeway_mask].to_records(index \u003d False)\n    return session_threeway\n\nX_all, Y_all \u003d processSubject(0)\nsession_all \u003d  processSessions(0)\nfor sub in range(1, 4):\n    x, y \u003d processSubject(sub)\n    session \u003d processSessions(sub)\n    X_all \u003d np.concatenate((X_all, x), axis \u003d 0)\n    Y_all \u003d np.concatenate((Y_all, y))\n    session_all \u003d np.concatenate((session_all, session))\n#\n\ndef peakData():\n    #np.ndarray\n    print(\"Shape of concatenated transformed fMRI data:\", X_all.shape)\n    print(\"Example row in the resulting 2D array: \", X_all[0])\n    #np.series\n    print(\"Length of concatenated labels:\", Y_all.shape)\n    print(\"First five labels: \",Y_all[0:5])\n    #pd.recarray\n    print(\"Shape of concatenated sessions: \", session_all.shape)\n    print(\"First fifteen tuples recording sessions: \", session_all[0:15])\n\npeakData()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Three-way classification with simple Dense Neural Net**  \nHere I use a simple neural network on the processed 2D dataset. This is a different type of ML model than the example SVM in the binary classification tutorial. ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nfrom keras import models\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n#three-way classification with NN\nX_train \u003d X_all[:800]\nX_val \u003d X_all[800:]\n# print(\"Length of training data: \", len(FHC_train))\n# print(\"Lenth of validation data: \", len(FHC_val))\n\n#need to one-hot encode the Y labels\nenc \u003d OneHotEncoder()\n#cited from https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\nY \u003d enc.fit_transform(Y_all[:, np.newaxis]).toarray()\nY_train \u003d Y[:800]\nY_val \u003d Y[800:]\n\n#DNN on all 4 subjects (1294 trials)\nmodel \u003d models.Sequential()\nmodel.add(Dense(32, input_dim \u003d 39912, activation\u003d\u0027relu\u0027))\nmodel.add(Dense(16, input_dim \u003d 32, activation\u003d\u0027relu\u0027))\nmodel.add(Dense(3, activation\u003d\u0027softmax\u0027))\nmodel.summary()\n\nmodel.compile(optimizer\u003d\u0027adam\u0027, loss\u003d\u0027categorical_crossentropy\u0027, metrics\u003d[\u0027accuracy\u0027])\nmodel.fit(X_train, Y_train, batch_size\u003d324, epochs\u003d10, verbose\u003d1)\nscore \u003d model.evaluate(X_val, Y_val)\n\nprint(\u0027Model with all four subjects data:\u0027)\nprint(\u0027Train loss:\u0027, score[0])\nprint(\u0027Train accuracy:\u0027, score[1])\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Three-way classification with Convolutional Neural Network**  \nAgain, CNN are a different class of model specializing in image classification. Here I use a form of the original brain scans instead of the masker-transformed voxel time series. ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-1-73367d5b72aa\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnilearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrop_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_imgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnilearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_stat_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mloadingData\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named \u0027loadingData\u0027"
          ],
          "ename": "ModuleNotFoundError",
          "evalue": "No module named \u0027loadingData\u0027",
          "output_type": "error"
        }
      ],
      "source": "from keras import layers, models\nfrom nilearn.image import crop_img, index_img, iter_img\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\ndef loadFilteredImages(sub):\n    behavioral \u003d pd.read_csv(haxby_dataset.session_target[sub], sep\u003d\" \")\n    conditions \u003d behavioral[\u0027labels\u0027]\n    print(\"length of all trials: \", len(conditions))\n    threeway_mask \u003d conditions.isin([\u0027face\u0027, \u0027house\u0027, \u0027cat\u0027])\n    images \u003d index_img(loadSubject(sub), threeway_mask)\n    return images\n\n#for subject 1, returned images are a set of 324 frames/trials, each containing 40 slices of 64*64 images\n#original shape: (40, 64, 64, 324)\nsubj1_images \u003d loadFilteredImages(0)\nimages \u003d np.empty((40, 64, 64))\n#use np.stack to reshape the 4D image array to (324, 40, 64, 64)\nimages \u003d np.stack([img.dataobj for i, img in enumerate(iter_img(subj1_images))])\ntrain_images \u003d images[:250]\nval_images \u003d images[250:]\nprint(\"Shape of input: \", train_images.shape)\nprint(\"Content of input: \", train_images)\n\n#need to one-hot encode the Y labels\nenc \u003d OneHotEncoder()\n#cited from https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\nY \u003d enc.fit_transform(conditions_threeway[:, np.newaxis]).toarray()\nY_train \u003d Y[:250]\nY_val \u003d Y[250:]\n\n#cited from https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.1-introduction-to-convnets.ipynb\nmodel \u003d models.Sequential()\nmodel.add(layers.Conv2D(32, kernel_size \u003d (3, 3), activation\u003d\u0027relu\u0027, input_shape\u003d(40, 64, 64)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation\u003d\u0027relu\u0027))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation\u003d\u0027relu\u0027))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation\u003d\u0027relu\u0027))\nmodel.add(layers.Dense(3, activation\u003d\u0027softmax\u0027))\nmodel.summary()\n\nmodel.compile(optimizer\u003d\u0027rmsprop\u0027,\n              loss\u003d\u0027categorical_crossentropy\u0027,\n              metrics\u003d[\u0027accuracy\u0027])\nmodel.fit(train_images, Y_train, epochs\u003d5, batch_size\u003d20)\n\ntest_loss, test_acc \u003d model.evaluate(val_images, Y_val)\n\nprint(\"Test loss: \", test_loss)\nprint(\"Test accuracy: \", test_acc)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Three-way classification on Subject 1 with SVM**  \nThis module extends upon the tutorial I followed by doing three-way instead of binary classification. ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "from sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.pipeline import Pipeline\nfrom nilearn import image\nfrom nilearn.plotting import plot_stat_map, show\nfrom sklearn.model_selection import LeaveOneGroupOut, cross_val_score\nimport matplotlib.pyplot as plt\n\n#cited from: https://nilearn.github.io/auto_examples/02_decoding/plot_haxby_anova_svm.html\n    # Define the dimension reduction to be used.\n    # Here we use a classical univariate feature selection based on F-test,\n    # namely Anova. When doing full-brain analysis, it is better to use\n    # SelectPercentile, keeping 5% of voxels\n    # (because it is independent of the resolution of the data).\nfeature_selection \u003d SelectPercentile(f_classif, percentile\u003d5)\nk_features \u003d SelectKBest(f_classif, k \u003d 7)\n\n# Output accuracy\n# Define the cross-validation scheme used for validation.\n# Here we use a LeaveOneGroupOut cross-validation on the session group\n# which corresponds to a leave-one-session-out\ndef modelAccuracy(model, X, conditions, groups):\n    cv \u003d LeaveOneGroupOut()\n\n    # Compute the prediction accuracy for the different folds (i.e. session)\n    cv_scores \u003d cross_val_score(model, X, conditions, cv\u003dcv, groups\u003dgroups)\n\n    # Return the corresponding mean prediction accuracy\n    classification_accuracy \u003d cv_scores.mean()\n\n    # Print the results\n    print(\"Classification accuracy: %.4f / Chance level: %f\" %\n          # (classification_accuracy, 1. / len(conditions.unique())))\n          (classification_accuracy, 1. / 3))\n\n#one-vs-the-rest linear kernel\n#cited from https://scikit-learn.org/stable/modules/svm.html#multi-class-classification\n#Pipeline ANOVA SVM with anova F-value, percetile feature selection. This is univariate feature selection\nprint(\"Fitting a linear SVC with Pipelined Anova f-value feature selection on subject 1 (324) trials: \")\nlin_svc \u003d LinearSVC()\nfacecathouse_svc \u003d Pipeline([(\u0027anova\u0027, feature_selection), (\u0027svc\u0027, lin_svc)])\nfacecathouse_svc.set_params(svc__C \u003d 10, svc__max_iter \u003d 2500)\nfacecathouse_svc.set_params(anova__percentile \u003d 3.3, svc__max_iter \u003d 750)\nfacecathouse_svc.fit(FHC, conditions_threeway)\n\nprint(\"Pipelined SVM with linear kernel accuracy: \")\nmodelAccuracy(facecathouse_svc, FHC, conditions_threeway, session_threeway)\n\ncross_validation \u003d cross_val_score(facecathouse_svc, FHC, conditions_threeway, cv \u003d 6, verbose \u003d 1)\nprint(\"Pipelined SVM with linear kernel cross validation score: \", cross_validation.mean())",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Three-way classification with SVM on all four subjects**  \nThis module extends upon previous work by including all four subjects with complete data in the analysis.  ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "X_all, Y_all \u003d processSubject(0)\nsession_all \u003d  processSessions(0)\nfor sub in range(1, 4):\n    x, y \u003d processSubject(sub)\n    session \u003d processSessions(sub)\n    X_all \u003d np.concatenate((X_all, x), axis \u003d 0)\n    Y_all \u003d np.concatenate((Y_all, y))\n    session_all \u003d np.concatenate((session_all, session))\n# fitting on all four subjects\nprint(\"Now fitting a linear SVC on all four subjects (1296 trials) instead of one subject:\")\nlin_svc1 \u003d LinearSVC()\nallSubs_svc \u003d Pipeline([(\u0027anova\u0027, feature_selection), (\u0027svc\u0027, lin_svc1)])\nallSubs_svc.set_params(anova__percentile \u003d 2.9, svc__max_iter \u003d 5000)\nallSubs_svc.fit(X_all, Y_all)\n\nprint(\"Linear SVM with all subjects accuracy: \")\nmodelAccuracy(allSubs_svc, X_all, Y_all, session_all)\n\ncross_validation \u003d cross_val_score(allSubs_svc, X_all, Y_all, cv \u003d 7, verbose \u003d 1)\nprint(\"Linear SVM with all subjects cross validation score: \", cross_validation.mean())",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\u003ch5\u003eImplications — Discuss the social and ethical implications of using the technologies you’ve chosen for your project\u003c/h5\u003e\nThis project will be using public datasets of fMRI data.\nIn addition, my hope is to learn about how statistical models can offer leverage to analysis of activity and connection \npatterns in the brain. Maching learning technologies are good at pattern-finding and navigating multi-demensional datasets \nIt can bring  benefits to researchers of the brain who are interested in understanding normal versus abnormal patterns of activity. \nRecently, there has been a surge of interest in using fMRI brain scans as a window to understand how our brain functions \nin a healthy or diminished way. The challenge for humans in detecting useful connections is an area of strength for \nmachine learning algorithms, specifically for neural networks and statistical models that are used in the NiLearn library.\nTherefore, I think these technologies will provide researchers with more insight into the underlying workings of our brain behind the mass of fMRI data. \nIn the future, our understanding of neurological disorders such as Alzheimer\u0027s may stand to benefit from bringing machine learning\ninto the lab to assist the human medical experts. \n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": " ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}